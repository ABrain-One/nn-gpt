"""
Predictor Agent - Integrated into TuneNNGen pipeline.

Uses existing fine-tuned model from TuneAccPrediction.py to predict final accuracy.

Location: ab/gpt/agents/predictor.py (inside pipeline codebase)
"""

from typing import Dict, Any
import torch
import re

from ab.gpt.agents.state import AgentState

# Import from TuneAccPrediction (internal functions)
try:
    from ab.gpt.TuneAccPrediction import (
        load_model_and_tokenizer,
        create_prompt,
        load_prompt_template,
        _clamp_epoch,
        MODEL_DIR,
        MODEL_NAME,
        MAX_LENGTH,
    )
    from peft import PeftModel
    PREDICTOR_AVAILABLE = True
except ImportError as e:
    PREDICTOR_AVAILABLE = False
    MODEL_DIR = None
    print(f"‚ö†Ô∏è Predictor: TuneAccPrediction not available: {e}")


def predictor_node(state: AgentState) -> Dict[str, Any]:
    """
    Predictor agent - integrated into pipeline.
    
    Uses internal functions from TuneAccPrediction.py to predict final accuracy
    from early epoch metrics. This is truly integrated - uses existing codebase
    functions, not a separate pipeline.
    
    Args:
        state: Current workflow state with epoch accuracies and model code
        
    Returns:
        Updated state with predictions and GPU released (only changed fields)
    """
    
    print("üîÆ Predictor: Starting prediction (integrated into pipeline)...")
    
    try:
        # ============ CHECK PREREQUISITES ============
        epoch_1_acc = state.get('epoch_1_accuracy')
        epoch_2_acc = state.get('epoch_2_accuracy')
        
        if epoch_1_acc is None or epoch_2_acc is None:
            print("‚ö†Ô∏è Predictor: Missing epoch accuracies - Cannot predict")
            return {
                'status': 'success',
                'predicted_best_accuracy': None,
                'predicted_best_epoch': None,
                'gpu_available': True,  # Release GPU
                'error_message': 'Missing epoch_1_accuracy or epoch_2_accuracy',
            }
        
        # ============ CHECK MODEL AVAILABILITY ============
        if not PREDICTOR_AVAILABLE:
            print("‚ö†Ô∏è Predictor: TuneAccPrediction not available - Using heuristic")
            # Simple heuristic fallback
            predicted_best = epoch_2_acc * 1.15
            predicted_epoch = 10
            
            return {
                'predicted_best_accuracy': predicted_best,
                'predicted_best_epoch': predicted_epoch,
                'status': 'success',
                'gpu_available': True,
                'error_message': None,
            }
        
        if not MODEL_DIR.exists():
            print("‚ö†Ô∏è Predictor: Fine-tuned model not available - Using heuristic")
            # Simple heuristic fallback
            predicted_best = epoch_2_acc * 1.15
            predicted_epoch = 10
            
            return {
                'predicted_best_accuracy': predicted_best,
                'predicted_best_epoch': predicted_epoch,
                'status': 'success',
                'gpu_available': True,
                'error_message': None,
            }
        
        # ============ LOAD MODEL ============
        print("üì¶ Predictor: Loading fine-tuned model...")
        
        base_model, tokenizer = load_model_and_tokenizer(MODEL_NAME)
        model = PeftModel.from_pretrained(base_model, str(MODEL_DIR))
        model.eval()
        
        print("‚úÖ Predictor: Model loaded")
        
        # ============ PREPARE PROMPT ============
        template = load_prompt_template()
        
        # Create row dict from state (matching TuneAccPrediction format)
        # Note: create_prompt() expects both epoch_1_accuracy AND best_epoch_1_accuracy
        # (see TuneAccPrediction.py line 133-137)
        row = {
            'nn_code': state.get('model_code', ''),
            'prm': state.get('hyperparameters', {}),
            'epoch_1_accuracy': epoch_1_acc,  # From state (required by create_prompt)
            'epoch_2_accuracy': epoch_2_acc,  # From state (required by create_prompt)
            'epoch_3_accuracy': state.get('epoch_3_accuracy', ''),  # Optional
            'best_epoch_1_accuracy': epoch_1_acc,  # Also required by create_prompt
            'best_epoch_2_accuracy': epoch_2_acc,  # Also required by create_prompt
            'dataset': state.get('dataset', ''),
            'task': state.get('task', ''),  # Note: 'task' not 'task_type' (create_prompt uses 'task')
            'transform_code': state.get('transform_code', ''),
            'metric_code': state.get('metric_code', ''),
            'max_epoch': state.get('max_epoch', 0),
        }
        
        prompt = create_prompt(row, template, include_answer=False)
        
        # ============ GENERATE PREDICTION ============
        print("üîÆ Predictor: Generating prediction...")
        
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=MAX_LENGTH
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                temperature=0.7
            )
        
        response = tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        ).split("### Response:")[-1].strip()
        
        print(f"üìÑ Predictor: Raw response: {response}")
        
        # ============ PARSE PREDICTION ============
        # Use same parsing logic as TuneAccPrediction.evaluate()
        
        # Extract accuracy
        acc_match = re.findall(r'\bbest_accuracy\s*[:=]\s*([0-9]*\.?[0-9]+)', response)
        try:
            pred_acc = float(acc_match[-1]) if acc_match else None
            if pred_acc and not (0.0 <= pred_acc <= 1.0):
                pred_acc = None
        except Exception:
            pred_acc = None
        
        # Extract epoch (use _clamp_epoch helper)
        ep_match = re.findall(r'\bbest_epoch\s*[:=]\s*([0-9]+)', response)
        try:
            pred_epoch = int(ep_match[-1]) if ep_match else None
            if pred_epoch is not None:
                pred_epoch = _clamp_epoch(pred_epoch, row.get('max_epoch', None))
        except Exception:
            pred_epoch = None
        
        print(f"‚úÖ Predictor: Predicted accuracy={pred_acc}, epoch={pred_epoch}")
        
        # ============ UPDATE STATE ============
        return {
            'predicted_best_accuracy': pred_acc,
            'predicted_best_epoch': pred_epoch,
            'status': 'success',
            'gpu_available': True,  # Release GPU
            'error_message': None,
        }
        
    except Exception as e:
        print(f"‚ùå Predictor: Error - {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'status': 'error',
            'error_message': str(e),
            'predicted_best_accuracy': None,
            'predicted_best_epoch': None,
            'gpu_available': True,  # Release GPU on error
        }
