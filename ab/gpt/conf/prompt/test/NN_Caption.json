{
  "improvement_captioning_structural": {
    "comment": ["Minimal, strict prompt for diverse captioning models with AdamW default."],
    "input_list": [{"para": "nn_code", "value": "nn_code"}],
    "addon_list": [
      {"para": "addon_nn_code_1", "value": "nn_code"},
      {"para": "addon_nn_code_2", "value": "nn_code"},
      {"para": "addon_nn_code_3", "value": "nn_code"},
      {"para": "addon_nn_code_4", "value": "nn_code"},
      {"para": "addon_nn_code_5", "value": "nn_code"},
      {"para": "addon_nn_code_6", "value": "nn_code"},
      {"para": "addon_nn_code_7", "value": "nn_code"},
      {"para": "addon_nn_code_8", "value": "nn_code"}
    ],
    "task": "img-captioning",
    "addon_task": "img-classification",
    "prompt": [
      "SYSTEM: Reply with EXACTLY ONE fenced code block that starts with ```python and ends with ``` . No analysis or prose.",
      "GOAL: Output a NEW image-captioning model file. Borrow 1â€“2 small ideas from the classification snippets (bottlenecks, SE/CBAM, depthwise/inverted residuals, pooling/stride tweaks, tiny transformer bits). Do NOT output a classifier.",
      "HARD RULES:",
      "- Define def supported_hyperparameters(): return {{'lr','momentum'}} (exactly).",
      "- Define class Net(nn.Module) with: __init__(in_shape,out_shape,prm,device), train_setup(prm), learn(train_data), forward(images,captions=None,hidden_state=None).",
      "- Expose decoder as self.rnn. self.rnn must implement: init_zero_hidden(batch,device)->(h0,c0) and forward(inputs,hidden_state,features=None)->(logits,hidden_state).",
      "- Teacher forcing: if captions.ndim==3 then captions=captions[:,0,:]; inputs=captions[:,:-1]; targets=captions[:,1:]; forward returns logits [B,T-1,V] and assert logits.shape[1]==inputs.shape[1].",
      "- I/O: in_channels=int(in_shape[1]); vocab_size=int(out_shape[0]); assert images.dim()==4; inputs.dtype is torch.long.",
      "- Encoder output is [B,R,H] (R>=1; use R=1 for global).",
      "- If using Transformer parts, set batch_first=True and ensure d_model % nhead==0.",
      "- Optimizer: use torch.optim.AdamW(self.parameters(), lr=prm['lr'], betas=(prm['momentum'], 0.999)). Must USE both lr and momentum. Loss: nn.CrossEntropyLoss(ignore_index=0).",
      "- Imports allowed: import torch; import torch.nn as nn; optional import torch.nn.functional as F; optional import math. No torchvision or other libs. No 'from torch import X'.",
      "- Keep params about <= 2e6. No undefined attributes.",
      "DIVERSITY:",
      "- Differ from ORIGINAL by >=3 structural choices (e.g., encoder block type, attention kind, decoder family {{GRU/LSTM/TransformerShim}}, hidden dim in {{384,512,640}}, normalization/activation, pooling/stride). Avoid reusing original variable names/order unless required by API.",
      "INSPIRATION:",
      "```python",
      "{addon_nn_code_1}",
      "```",
      "```python",
      "{addon_nn_code_2}",
      "```",
      "```python",
      "{addon_nn_code_3}",
      "```",
      "```python",
      "{addon_nn_code_4}",
      "```",
      "```python",
      "{addon_nn_code_5}",
      "```",
      "```python",
      "{addon_nn_code_6}",
      "```",
      "```python",
      "{addon_nn_code_7}",
      "```",
      "```python",
      "{addon_nn_code_8}",
      "```",
      "ORIGINAL CAPTIONING CODE:",
      "```python",
      "{nn_code}",
      "```"
    ]
  }
}
