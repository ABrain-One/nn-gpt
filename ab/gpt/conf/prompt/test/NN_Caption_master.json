{
  "captioning_master": {
    "comment": [
      "Master prompt: emits a runnable captioning model with strict API. The LLM chooses decoder type (Transformer, LSTM, GRU) and hidden sizes/heads based on the classification inspirations and encoder.",
      "Ensures the model is trained using teacher forcing, returns proper shapes, and avoids undefined variables or untested custom attention modules."
    ],
    "input_list": [
      {"para": "nn_code", "value": "nn_code"}
    ],
    "addon_list": [
      {"para": "addon_nn_code_1", "value": "nn_code"},
      {"para": "addon_nn_code_2", "value": "nn_code"},
      {"para": "addon_nn_code_3", "value": "nn_code"},
      {"para": "addon_nn_code_4", "value": "nn_code"},
      {"para": "addon_nn_code_5", "value": "nn_code"}
    ],
    "task": "img-captioning",
    "addon_task": "img-classification",
    "prompt": [
      "SYSTEM: Output exactly one fenced Python code block and nothing else.  Do not include prose or multiple code blocks.",
      "",
      "GOAL: Your task is to generate a high-performance image captioning model by taking inspiration from classification model code blocks, and by making safe, meaningful structural tweaks to the target captioning model.",
      "Do NOT output a classifier.  Produce exactly one full, runnable Python file that conforms to the API below.",
      "We prioritize high BLEU on the COCO dataset while keeping the model simple, reliable, and easy to train.  Model size and speed are constraints.",
      "",
      "MANDATORY API (must pass validation):",
      "- def supported_hyperparameters(): return {{'lr','momentum'}}",
      "- class Net(nn.Module) with methods:",
      "    - __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device)",
      "    - train_setup(self, prm)",
      "    - learn(self, train_data)",
      "    - forward(self, images, captions=None, hidden_state=None) -> (logits, hidden_state).",
      "- Expose the decoder as self.rnn and implement:",
      "    init_zero_hidden(batch: int, device: torch.device) -> (h0, c0).  For Transformer decoders return two empty tensors for compatibility.",
      "    forward(inputs, hidden_state, features=None) -> (logits, hidden_state).",
      "",
      "REUSE THE ENCODER BACKBONE:",
      "- Remove the classification head from the chosen classification blocks.  Keep the convolutional backbone for the encoder.  Produce a feature tensor [B,1,H] via AdaptiveAvgPool2d and a Linear layer.  Hidden dimension H must be ≥640 (e.g. 640 or 768).",
      "",
      "DECODER CHOICE:",
      "- YOU decide which decoder architecture best suits your chosen encoder and classification inspiration:",
      "    * Use an LSTM or GRU with an embedding layer and hidden size ≥640 (e.g. 640 or 768).  Initialise hidden state from the encoder features or pass features at each step.  You may include additive or dot-product attention on the encoder feature.",
      "    * OR a 1-layer nn.TransformerDecoder (batch_first=True) with MultiheadAttention; choose num_heads dividing hidden_size (e.g. hidden_size=768, num_heads=8 or 12).  Use a simple positional encoding (sinusoidal or learnable embedding).  Cross-attend the decoder to the encoder memory.",
      "- You MUST decide the best hidden size and number of heads yourself; do not expect these values in the prompt.",
      "",
      "TEACHER FORCING & SHAPES:",
      "- If captions.ndim==3, set captions=captions[:,0,:].",
      "- inputs = captions[:,:-1]; targets = captions[:,1:].",
      "- forward() must return (logits, hidden_state) where logits has shape [B,T-1,vocab_size].",
      "- Shape Sentry: assert images.dim()==4; assert logits.shape[1]==inputs.shape[1]; assert logits.shape[-1]==vocab_size.",
      "- in_channels = int(in_shape[1]); vocab_size = int(out_shape[0]).",
      "",
      "TRAINING SETUP:",
      "- In train_setup(): call self.to(self.device); set self.criteria to nn.CrossEntropyLoss(ignore_index=0); use torch.optim.AdamW(self.parameters(), lr=prm['lr'], betas=(prm['momentum'], 0.999)).",
      "- In learn(): implement a loop over train_data; call logits,_ = self.forward(images,captions,None); compute loss as CrossEntropyLoss on logits.reshape(-1,V) and targets.reshape(-1); zero grad, backward, clip grads, step optimizer.  No scheduler unless simple (e.g. ReduceLROnPlateau).",
      "",
      "SAFE EDITS ONLY:",
      "- Reuse the base captioning skeleton.  Only modify: encoder blocks (e.g. bottleneck 1x1-3x3-1x1, SE/CBAM), decoder type and hidden dims/head counts, and simple attention on encoder features.",
      "- Do NOT create complex custom cross-attention or positional encodings.  Use nn.MultiheadAttention and PositionalEncoding or a simple embedding for positions.",
      "- Do NOT import torchvision or pretrained weights.  Define all layers in this file.",
      "- Avoid referencing undefined variables or copying classification variables that are irrelevant.",
      "",
      "DIVERSITY REQUIREMENTS:",
      "- You MUST make at least three structural changes relative to the original captioning model: change the encoder block type, choose a different decoder family, vary hidden dimensions (≥640), add SE/CBAM or depthwise blocks, etc.",
      "- Randomise your choices per run to encourage a wide variety of models.  But keep within the safe modifications above.",
      "",
      "BLEU-ORIENTED GUIDANCE:",
      "- Larger hidden sizes and multi-head attention (when using Transformer) generally improve BLEU.  Hidden sizes in {{640, 768}} and num_heads in {{8, 12}} are good starting points.",
      "- Adding Squeeze-and-Excitation (SE) or CBAM to the encoder can help while staying within budget.",
      "- Keep dropout modest (0.1–0.3) in the decoder.  Clip gradients at 3.0.",
      "- Keep parameter count around 2 million.  Do not add untested modules.",
      "",
      "INSPIRATION CLASSIFICATION MODELS:",
      "```python",
      "{addon_nn_code_1}",
      "```",
      "```python",
      "{addon_nn_code_2}",
      "```",
      "```python",
      "{addon_nn_code_3}",
      "```",
      "```python",
      "{addon_nn_code_4}",
      "```",
      "```python",
      "{addon_nn_code_5}",
      "```",
      "",
      "ORIGINAL CAPTIONING CODE (reference only):",
      "```python",
      "{nn_code}",
      "```",
      "",
      "WRITE THE FINAL RUNNABLE FILE NOW.  Do not include extra commentary."
    ]
  }
}
